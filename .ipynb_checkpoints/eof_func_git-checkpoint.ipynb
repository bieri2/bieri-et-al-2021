{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Allow anonymous logging usage to help improve CDAT(you can also set the environment variable CDAT_ANONYMOUS_LOG to yes or no)? [yes]/no:  no\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------- Import necessary modules\n",
    "from ipynb.fs.full.read_MERRA import *\n",
    "from ipynb.fs.full.read_era5 import *\n",
    "from ipynb.fs.full.eof_func import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import xesmf as xe\n",
    "import xarray as xr\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if month is in SONDJF\n",
    "def is_sondjf(month):\n",
    "    return (month >= 9) | (month <= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if month is in ONDJFM\n",
    "def is_ondjfm(month):\n",
    "    return (month >= 10) | (month <= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_std_anom(data, resolution):\n",
    "    if resolution=='monthly':\n",
    "        #Calculate monthly climatological mean and standard deviation\n",
    "        data_mean = data.groupby('time.month').mean(dim='time')\n",
    "        data_std  = data.groupby('time.month').std(dim='time')\n",
    "\n",
    "        #Compute standardized anomalies\n",
    "        data_std_anom = xr.apply_ufunc(lambda x, m, s: (x - m) / s, data.groupby('time.month'), data_mean, data_std)\n",
    "        \n",
    "    elif resolution=='daily':\n",
    "        #Calculate daily climatological mean and standard devation\n",
    "        data_mean = data.groupby('time.dayofyear').mean(dim='time')\n",
    "        data_std  = data.groupby('time.dayofyear').mean(dim='time')\n",
    "        \n",
    "        #Compute standardized anomalies\n",
    "        data_std_anom = xr.apply_ufunc(lambda x, m, s: (x - m) / s, data.groupby('time.dayofyear'), data_mean, data_std)\n",
    "        \n",
    "    return data_std_anom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_comp(data, pcs):\n",
    "    # Get composites for time steps above and below 1 std\n",
    "    comp_p = np.nanmean(data[pcs > np.std(pcs), :, :], axis=0)\n",
    "    comp_n = np.nanmean(data[pcs < (-1*np.std(pcs)), :, :], axis=0)\n",
    "\n",
    "    return comp_p, comp_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will use the Python eofs package to perform standard or extended EOF analysis using the given input data\n",
    "# \"lats\" refers to the array of latitude values associated with the input data - should be the same for all input datasets \n",
    "# \"data\" is the data to be used in EOF analysis - should be a list with each element being an array, max 3 input arrays\n",
    "# \"neofs\" refers to the number of EOFs to be returned\n",
    "# \"npcs\" is the number of PC time series to be returned \n",
    "\n",
    "# Output from this function is a dictionary with EOF spatial patterns corresponding to the input data (\"eofs\" or \"eofs1/2/3\"),\n",
    "# PC values (\"pcs\", variance explained by each EOF (\"var\"), and eigenvalues (\"eigs\") for each EOF (multivariate only).\n",
    "\n",
    "# Author: Carolina Bieri (bieri2@illinois.edu)\n",
    "\n",
    "def calc_eof(lats,data,neofs,npcs,multi=False):\n",
    "    # Import necessary modules\n",
    "    from eofs.standard import Eof\n",
    "    from eofs.multivariate.standard import MultivariateEof\n",
    "    import numpy as np\n",
    "    \n",
    "    # Define latitude weights \n",
    "    coslat = np.cos(np.deg2rad(lats))\n",
    "    wgts   = np.sqrt(coslat)[..., np.newaxis]\n",
    "    \n",
    "    # Do this if not multivariate EOF\n",
    "    if not multi:\n",
    "        # Create an EOF solver to do the EOF analysis. Square-root of cosine of\n",
    "        # latitude weights are applied before the computation of EOFs.\n",
    "        solver    = Eof(data, wgts)\n",
    "        \n",
    "        # Get EOFs, PCs, and variance fractions\n",
    "        # EOF scaling = 1 means that eigenvectors are divided by the sq root of eigenvalue\n",
    "        # EOF scaling = 2 means that eigenvectors are multiplied by the sq root of eigenvalue\n",
    "        eof       = solver.eofs(neofs=neofs, eofscaling=2)\n",
    "        pc        = solver.pcs(npcs=npcs, pcscaling=1)\n",
    "        variance_fraction = solver.varianceFraction(10)\n",
    "        \n",
    "        # Define dictionary to hold output \n",
    "        calc = {\"eofs\" : eof,\n",
    "                \"pcs\"  : pc,\n",
    "                \"var\" : variance_fraction}\n",
    "        \n",
    "    # Do this if multivariate EOF\n",
    "    else:\n",
    "        # Create a multivariate EOF solver to do the EOF analysis. Square-root of cosine of\n",
    "        # latitude weights are applied before the computation of EOFs.\n",
    " \n",
    "        # Do this if 2 datasets are passed to function\n",
    "        if len(data) == 2:\n",
    "            msolver      = MultivariateEof(data, weights=[wgts,wgts])\n",
    "            eofs1, eofs2 = msolver.eofs(neofs=neofs, eofscaling=2)\n",
    "            pcs          = msolver.pcs(npcs=npcs, pcscaling=1)\n",
    "            variance_fraction = msolver.varianceFraction(10)\n",
    "            eigs         = msolver.eigenvalues(neigs=neofs)\n",
    "        \n",
    "            calc = {\"eofs1\" : eofs1,\n",
    "                    \"eofs2\" : eofs2,\n",
    "                    \"pcs\"   : pcs,\n",
    "                    \"var\"   : variance_fraction,\n",
    "                    \"eigs\"  : eigs}\n",
    "        \n",
    "        # Do this if 3 datasets are passed to function\n",
    "        if len(data) == 3:\n",
    "            msolver     = MultivariateEof(data, weights=[wgts,wgts,wgts])\n",
    "            eofs1, eofs2, eofs3 = msolver.eofs(neofs=neofs, eofscaling=2)\n",
    "            pcs         = msolver.pcs(npcs=npcs, pcscaling=1)\n",
    "            variance_fraction = msolver.varianceFraction(10)\n",
    "            eigs        = msolver.eigenvalues(neigs=neofs)\n",
    "        \n",
    "            calc = {\"eofs1\" : eofs1,\n",
    "                    \"eofs2\" : eofs2,\n",
    "                    \"eofs3\" : eofs3,\n",
    "                    \"pcs\"   : pcs,\n",
    "                    \"var\"   : variance_fraction, \n",
    "                    \"eigs\"  : eigs}\n",
    "        \n",
    "        \n",
    "    return calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will plot EOF output from the above function in a standard format\n",
    "# \"lons\" is the lon array corresponding to the input EOF arrays\n",
    "# \"lats\" is the lat array corresponding to the input EOF arrays \n",
    "# \"eofs\" is an array containing the EOF patterns to be plotted\n",
    "# \"neofs\" is the number of EOFs to be plotted \n",
    "# \"vmin\" is the lowest value to be plotted\n",
    "# \"vmax\" is the highest values to be plotted\n",
    "# \"sm\" should be set to True if SM EOFs are being plotted\n",
    "# \"pcp\" should be set to True if precip EOFs are being plotted\n",
    "\n",
    "# One plot is created for each EOF\n",
    "\n",
    "def plot_eof(lons,lats,eofs,neofs,vmin,vmax, var, sm=False, pcp=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.basemap import Basemap\n",
    "    import matplotlib.ticker as ticker\n",
    "    import numpy as np\n",
    "    \n",
    "    # Define subplots\n",
    "    fig, axes = plt.subplots(1, neofs, figsize=(8,4))\n",
    "    \n",
    "    # Plot each EOF\n",
    "    for i in range(neofs):\n",
    "        bins = np.arange(vmin,vmax+0.1,0.1)\n",
    "        number_bins = len(bins)-1\n",
    "\n",
    "        # Create map\n",
    "        map = Basemap(resolution='l', projection='cyl', llcrnrlon=min(lons), urcrnrlon=max(lons), \n",
    "                     llcrnrlat=min(lats), urcrnrlat=max(lats), lat_0=0, lon_0=0, ax=axes[i])\n",
    "        \n",
    "        lon_plt, lat_plt = np.meshgrid(np.asarray(lons), np.asarray(lats))\n",
    "    \n",
    "        xi, yi = map(np.asarray(lon_plt), np.asarray(lat_plt))\n",
    "        \n",
    "        # Set colormap based on variable\n",
    "        if sm:\n",
    "            cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#6c3811\",\"white\",\"#21A926\"])\n",
    "        elif pcp:\n",
    "            cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#6c3811\",\"white\",\"#179DC9\"])\n",
    "        else: \n",
    "            cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#131DBF\",\"white\",\"#C91717\"])\n",
    "        \n",
    "        # Plot the current EOF\n",
    "        fill = map.pcolormesh(xi, yi, eofs[i,:,:],cmap=cmap._resample(number_bins),vmin=vmin,vmax=vmax, alpha=1)\n",
    "    \n",
    "        map.drawcoastlines()\n",
    "        map.drawcountries()\n",
    "        \n",
    "        # Add colorbar with tickmarks in scientific notation\n",
    "        cb = fig.colorbar(fill, orientation='horizontal',ax=axes[i], pad=0.05, ticks=[-0.65, -0.35, 0, 0.35, 0.65])\n",
    "        cb.ax.tick_params(labelsize=14)\n",
    "        # Set number of tick marks on colorbar\n",
    "        #tick_locator = ticker.MaxNLocator(nbins=13)\n",
    "        #cb.locator = tick_locator\n",
    "        #cb.update_ticks()\n",
    "        \n",
    "        # Add title to each plot\n",
    "        axes[i].set_title('EOF {:1d} - {:2.2%}'.format(i+1, var[i]), fontsize=16)\n",
    "        \n",
    "    #plt.figure(figsize=(8,8))\n",
    "    #plt.bar(np.arange(0,10,1),var*100, color='cornflowerblue')\n",
    "    #plt.xticks(np.arange(0,10,1), ('1','2','3','4','5','6','7','8','9', \\\n",
    "    #       '10'), size=20)\n",
    "    #plt.xlabel('EOF', size=20)\n",
    "    #plt.ylabel('Percent of variance explained', size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_monvar(pc):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    mon_var = np.zeros(6)\n",
    "\n",
    "    for i in range(6):\n",
    "        mon_var[i] = np.sum(np.square(pc[i::6]))\n",
    "\n",
    "    mon_var = (mon_var/np.size(pc))*100\n",
    "    #slope, intercept, r_value, p_value, std_err = ss.linregress(np.linspace(1,5,5),mon_var[0:5])\n",
    "\n",
    "    #rand_slopes = np.zeros(500)\n",
    "    #for N in range(500):\n",
    "    #    rand_var = np.zeros(6)\n",
    "    #    shuffled = pc/np.sqrt(eig)\n",
    "    #    np.random.shuffle(shuffled)\n",
    "    #    for i in range(6):\n",
    "    #        rand_var[i] = rand_var[i] + np.sum(np.square(shuffled[i::6]))\n",
    "\n",
    "    #    rand_var = rand_var/np.size(shuffled)*100\n",
    "    #    rand_slopes[N], test_intercept, r_value, p_value, std_err = ss.linregress(np.linspace(1,5,5),rand_var[0:5])\n",
    "    \n",
    "    #pvalue = ss.percentileofscore(rand_slopes,slope)\n",
    "   \n",
    "    plt.rcParams[\"axes.linewidth\"] = 1.25\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    end = np.size(mon_var)\n",
    "    xvector = np.linspace(1,end,end)\n",
    "    \n",
    "    ax.plot(xvector,mon_var,lw=2, color='lightsteelblue')\n",
    "    ax.scatter(xvector,mon_var, zorder=10, color='cornflowerblue',edgecolor='royalblue',s=50)\n",
    "    \n",
    "    ax.set_ylabel('% variance',fontsize = '16', fontname='Cantarell')\n",
    "    ax.set_xlabel('Month',fontsize = '16', fontname='Cantarell')\n",
    "    #plt.title(title + ' (slope=' + str(np.round(slope,4)) + ', p-value=' + str(np.round(pvalue,2)) + ')', fontsize = '14')\n",
    "    ticks = xvector\n",
    "    labels = ['Sep','Oct','Nov','Dec','Jan','Feb']\n",
    "\n",
    "    plt.xticks(ticks,labels,rotation=0,fontsize=12, fontname='Cantarell')\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    #ax.plot(xvector,slope*xvector+intercept,'r--',lw=0.75, alpha=0.9)\n",
    "    \n",
    "    #plt.yticks(fontsize=14)\n",
    "    #plt.xticks(fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pc(x,pcs,ymin,ymax,yearbegin=1980,yearend=2016,lw=3):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    from Carolina_Fonts_v1 import Open_Sans\n",
    "    Open_Sans()\n",
    "\n",
    "    # Plot time series of PC 1 and PC 2\n",
    "    plt.subplots(2,1,figsize=(20,7),sharex=True)\n",
    "\n",
    "    # Plot PC 1 on first subplot\n",
    "    plt.subplot(211)\n",
    "    plt.plot(x,pcs[:,0], color='skyblue', linewidth=lw)\n",
    "    plt.title('a) Principal component time series - EOF 1' , size=26)\n",
    "    plt.xticks(fontsize=24)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xlim(yearbegin,yearend)\n",
    "    plt.ylim(ymin,ymax)\n",
    "    plt.hlines(np.std(pcs[:,0]),np.min(x),np.max(x), linestyles='dashed')\n",
    "    plt.scatter(x[(pcs[:,0]>np.std(pcs[:,0])).nonzero()],pcs[:,0][pcs[:,0]>np.std(pcs[:,0])],zorder=10, color='black', s=20)\n",
    "\n",
    "    # Plot PC 2 on second subplot\n",
    "    plt.subplot(212)\n",
    "    plt.plot(x,pcs[:,1], color='cornflowerblue', linewidth=lw)\n",
    "    plt.title('b) Principal component time series - EOF 2', size=26)\n",
    "    plt.xticks(fontsize=24)\n",
    "    plt.yticks(fontsize=20)\n",
    "    plt.xlim(yearbegin,yearend)\n",
    "    plt.ylim(ymin,ymax)\n",
    "    plt.hlines(np.std(pcs[:,1]),np.min(x),np.max(x), linestyles='dashed')\n",
    "    plt.scatter(x[(pcs[:,1]>np.std(pcs[:,1])).nonzero()],pcs[:,1][pcs[:,1]>np.std(pcs[:,1])],zorder=10, color='black', s=20)\n",
    "\n",
    "    \n",
    "    plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.4)\n",
    "    \n",
    "    #pc_mean = np.zeros((6,2))\n",
    "    #pcs = np.abs(pcs[2:,:])\n",
    "    #for i in range(6):\n",
    "    #    pc_mean[i,0] = np.mean(pcs[i::6,0], axis=0)\n",
    "    #    pc_mean[i,1] = np.mean(pcs[i::6,1], axis=0)\n",
    "        \n",
    "    #end = np.size(pc_mean[:,0])\n",
    "    #xvector = np.linspace(1,end,end)\n",
    "        \n",
    "    #plt.figure()\n",
    "    #plt.plot(xvector,pc_mean[:,0], 'red')\n",
    "    #plt.plot(xvector,pc_mean[:,1], 'blue')\n",
    "\n",
    "    #labels = ['Sep','Oct','Nov','Dec','Jan','Feb']\n",
    "    #plt.xticks(xvector,labels,rotation=0,fontsize=12, fontname='Cantarell')\n",
    "    #plt.yticks(fontsize=12)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comp(lon,lat,comp_p,comp_n,title,cmap,eofnum,interval,ticks,vmin=-1.5,vmax=1.5,U=[],V=[],stride=6, scale=35,plot=False, filename=''):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.basemap import Basemap\n",
    "    import numpy as np\n",
    "    \n",
    "    from Carolina_Fonts_v1 import Open_Sans\n",
    "    Open_Sans()\n",
    "    \n",
    "    bins = np.arange(vmin,vmax+interval,interval)\n",
    "    number_bins = len(bins)-1\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    map = Basemap(resolution='l', projection='cyl', llcrnrlon=min(lon), urcrnrlon=max(lon), \n",
    "                     llcrnrlat=min(lat), urcrnrlat=max(lat), lat_0=0, lon_0=0)\n",
    "        \n",
    "    lon_plt, lat_plt = np.meshgrid(np.asarray(lon), np.asarray(lat))\n",
    "    \n",
    "    xi, yi = map(np.asarray(lon_plt), np.asarray(lat_plt))\n",
    "    \n",
    "    if (len(U) > 0) and (len(V) > 0):\n",
    "        lon_coarse, lat_coarse = np.meshgrid(np.asarray(lon[::stride]), np.asarray(lat[::stride]))\n",
    "        xi_coarse, yi_coarse   = map(np.asarray(lon_coarse),np.asarray(lat_coarse))\n",
    "        plt.quiver(xi_coarse, yi_coarse, U[0], V[0], color='black',zorder=10, scale=scale, headlength=3.0, headaxislength=3.0)\n",
    "    \n",
    "    fill = map.pcolormesh(xi, yi, comp_p,cmap=cmap._resample(number_bins),vmin=vmin,vmax=vmax, alpha=1)\n",
    "    fill.set_edgecolor('face')\n",
    "\n",
    "    map.drawcoastlines()\n",
    "    map.drawcountries()\n",
    "    cb = plt.colorbar(fill, orientation='horizontal',ticks=ticks, pad=0.05)\n",
    "    cb.set_label(label='Standardized anomaly', fontsize=24)\n",
    "    cb.ax.tick_params(labelsize=20)\n",
    "    \n",
    "    plt.title(title, fontsize=26)\n",
    "    \n",
    "    \n",
    "    plt.savefig(filename + '.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    #plt.figure(figsize=(10,10))\n",
    "    #map = Basemap(resolution='l', projection='cyl', llcrnrlon=min(lon), urcrnrlon=max(lon), \n",
    "    #                 llcrnrlat=min(lat), urcrnrlat=max(lat), lat_0=0, lon_0=0)\n",
    "    \n",
    "    #if (len(U) > 0) and (len(V) > 0):\n",
    "    #    plt.quiver(xi_coarse, yi_coarse, U[1], V[1], color='black', zorder=10, scale=scale, headlength=3.0, headaxislength=3.0)\n",
    "        \n",
    "    #fill = map.pcolormesh(xi, yi, comp_n,cmap=cmap._resample(number_bins),vmin=vmin,vmax=vmax, alpha=0.9)\n",
    "    #fill.set_edgecolor('face')\n",
    "\n",
    "    #map.drawcoastlines()\n",
    "    #map.drawcountries()\n",
    "    #cb = plt.colorbar(fill, orientation='horizontal')\n",
    "    #cb.set_label(label='Standardized anomaly', fontsize=16)\n",
    "    #cb.ax.tick_params(labelsize=12)\n",
    "    #plt.title('Composite for EOF '+ str(eofnum) + ' - PC < -1*STD', fontsize=22)\n",
    "    #if plot:\n",
    "    #    plt.savefig(filename+'ghwindcompn.png', dpi=300)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def do_eof(T2M, SM, pcp, resolution, beginyear, nyears, filename, GH1=None, GH2=None, U1=None, V1=None, U2=None, V2=None, LH=None,\n",
    "           SH=None, eoflim1=-1.0,eoflim2=1.0,meoflim1=-0.65,meoflim2=0.65, clim1=-1.0,clim2=1.0, ancillary=True, opp=False):\n",
    "    \n",
    "    import seaborn as sns\n",
    "    from Carolina_Fonts_v1 import Open_Sans\n",
    "    \n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    Open_Sans()\n",
    "    \n",
    "    ###################################\n",
    "    # -------------------------------- Calculate standardized anomalies\n",
    "    T2M = calc_std_anom(T2M,resolution)\n",
    "    SM  = calc_std_anom(SM,resolution)\n",
    "    pcp = calc_std_anom(pcp,resolution)\n",
    "    \n",
    "    if ancillary:\n",
    "        GH1 = calc_std_anom(GH1,resolution)\n",
    "        GH2 = calc_std_anom(GH2,resolution)\n",
    "        U1  = calc_std_anom(U1,resolution)\n",
    "        U2  = calc_std_anom(U2,resolution)\n",
    "        V1  = calc_std_anom(V1,resolution)\n",
    "        V2  = calc_std_anom(V2,resolution)\n",
    "        LH  = calc_std_anom(LH,resolution)\n",
    "        SH  = calc_std_anom(SH,resolution)\n",
    "    ###################################\n",
    "    \n",
    "    \n",
    "    ###################################\n",
    "    # -------------------------------- Select wanted time steps and lag data\n",
    "    if resolution == 'monthly':\n",
    "        # Select data only from SONDJF (non-lagged) or ONDJFM (lagged)\n",
    "        # Remove last time step from T2M, SM, U1, V1, LH, SH, and GH1 \n",
    "        # Remove first time step from pcp, U2, V2, and GH2 \n",
    "        # This will ensure that the time dimension for precip is lagged by one month \n",
    "        T2M = T2M.sel(time=is_sondjf(T2M['time.month']))[:-1,:,:] \n",
    "        SM  = SM.sel(time=is_sondjf(SM['time.month']))[:-1,:,:]\n",
    "        \n",
    "        pcp = pcp.sel(time=is_ondjfm(pcp['time.month']))[1:,:,:]\n",
    "        \n",
    "        if ancillary:\n",
    "            GH1 = GH1.sel(time=is_sondjf(GH1['time.month']))[:-1,:,:]\n",
    "            U1  = U1.sel(time=is_sondjf(U1['time.month']))[:-1,:,:]\n",
    "            V1  = V1.sel(time=is_sondjf(V1['time.month']))[:-1,:,:]\n",
    "            LH  = LH.sel(time=is_sondjf(LH['time.month']))[:-1,:,:]\n",
    "            SH  = SH.sel(time=is_sondjf(SH['time.month']))[:-1,:,:]\n",
    "    \n",
    "            U2  = U2.sel(time=is_ondjfm(U2['time.month']))[1:,:,:]\n",
    "            V2  = V2.sel(time=is_ondjfm(V2['time.month']))[1:,:,:]\n",
    "            GH2 = GH2.sel(time=is_ondjfm(GH2['time.month']))[1:,:,:]\n",
    "        \n",
    "    elif resolution == 'daily':\n",
    "        # Get pcp data for March 1st and 2nd of all years\n",
    "        pcp_mar1 = pcp.sel(time=(pcp['time.month']==3)&(pcp['time.day']==1))\n",
    "        pcp_mar2 = pcp.sel(time=(pcp['time.month']==3)&(pcp['time.day']==2))\n",
    "\n",
    "        # Get GH2 data for March 1st and 2nd of all years\n",
    "        GH2_mar1 = GH2.sel(time=(GH2['time.month']==3)&(GH2['time.day']==1))\n",
    "        GH2_mar2 = GH2.sel(time=(GH2['time.month']==3)&(GH2['time.day']==2))\n",
    "\n",
    "        # Get U2 data for March 1st and 2nd of all years\n",
    "        U2_mar1 = U2.sel(time=(U2['time.month']==3)&(U2['time.day']==1))\n",
    "        U2_mar2 = U2.sel(time=(U2['time.month']==3)&(U2['time.day']==2))\n",
    "\n",
    "        # Get V2 data for March 1st and 2nd of all years\n",
    "        V2_mar1 = V2.sel(time=(V2['time.month']==3)&(V2['time.day']==1))\n",
    "        V2_mar2 = V2.sel(time=(V2['time.month']==3)&(V2['time.day']==2))\n",
    "        \n",
    "        # Remove last two time steps from T2M, SM, U1, V1, LH, SH, and GH1 \n",
    "        # Remove first two time steps from pcp, U2, V2, and GH2\n",
    "        # This will ensure that the time dimension for precip is lagged by two days \n",
    "        T2M = T2M.sel(time=is_sondjf(T2M['time.month']))[:-2,:,:]\n",
    "        SM  = SM.sel(time=is_sondjf(SM['time.month']))[:-2,:,:]\n",
    "        GH1 = GH1.sel(time=is_sondjf(GH1['time.month']))[:-2,:,:]\n",
    "        U1  = U1.sel(time=is_sondjf(U1['time.month']))[:-2,:,:]\n",
    "        V1  = V1.sel(time=is_sondjf(V1['time.month']))[:-2,:,:]\n",
    "        LH  = LH.sel(time=is_sondjf(LH['time.month']))[:-2,:,:]\n",
    "        SH  = SH.sel(time=is_sondjf(SH['time.month']))[:-2,:,:]\n",
    "        \n",
    "        U2  = U2.sel(time=is_sondjf(U850['time.month']))[2:,:,:]\n",
    "        V2  = V2.sel(time=is_sondjf(V850['time.month']))[2:,:,:]\n",
    "        GH2 = GH2.sel(time=is_sondjf(GH2['time.month']))[2:,:,:]\n",
    "        pcp = pcp.sel(time=is_sondjf(pcp['time.month']))[2:,:,:]\n",
    "        \n",
    "        # Artifically set first two time steps in September to the precip data from the first two days in March\n",
    "        # This is to ensure that all lags are consecutive and are not affected by gaps between seasons\n",
    "        pcp_temp = np.asarray(pcp)\n",
    "        pcp_temp[(pcp['time.day']==1)&(pcp['time.month']==9),:,:] = pcp_mar1\n",
    "        pcp_temp[(pcp['time.day']==2)&(pcp['time.month']==9),:,:] = pcp_mar2\n",
    "\n",
    "        GH2_temp = np.asarray(GH2)\n",
    "        GH2_temp[(GH2['time.day']==1)&(GH2['time.month']==9),:,:] = GH2_mar1\n",
    "        GH2_temp[(GH2['time.day']==2)&(GH2['time.month']==9),:,:] = GH2_mar2\n",
    "\n",
    "        U2_temp = np.asarray(U2)\n",
    "        U2_temp[(U2['time.day']==1)&(U2['time.month']==9),:,:] = U2_mar1\n",
    "        U2_temp[(U2['time.day']==2)&(U2['time.month']==9),:,:] = U2_mar2\n",
    "\n",
    "        V2_temp = np.asarray(V2)\n",
    "        V2_temp[(V2['time.day']==1)&(V2['time.month']==9),:,:] = V2_mar1\n",
    "        V2_temp[(V2['time.day']==2)&(V2['time.month']==9),:,:] = V2_mar2\n",
    "    ###################################\n",
    "\n",
    "    # Get lons and lats of domain\n",
    "    lon_small = T2M.coords['lon'].values\n",
    "    lat_small = T2M.coords['lat'].values\n",
    "    \n",
    "    if ancillary:\n",
    "        # Get lons and lats for GH and wind data (larger domain)\n",
    "        lon_large = GH1.coords['lon'].values\n",
    "        lat_large = GH1.coords['lat'].values\n",
    "    \n",
    "    \n",
    "    ###################################\n",
    "    # -------------------------------- Detrend data\n",
    "    \n",
    "    # Get indices of NaNs in SM array\n",
    "    where_nan_sm = np.isnan(np.array(SM)).nonzero()\n",
    "    # Fill NaNs with zeros\n",
    "    # This must be done in order to detrend using signal.detrend\n",
    "    SM_temp_dt   = SM.fillna(0.0)\n",
    "    \n",
    "    # Detrend T2M, pcp, SM data\n",
    "    t2m_dt = signal.detrend(T2M, axis=0)\n",
    "    pcp_dt = signal.detrend(pcp, axis=0)\n",
    "    sm_dt  = signal.detrend(SM_temp_dt, axis=0)\n",
    "    \n",
    "    # Replace NaNs in SM array\n",
    "    sm_dt[where_nan_sm] = np.NaN\n",
    "    \n",
    "    if ancillary:\n",
    "        # Get indices of NaNs in arrays\n",
    "        where_nan_u1 = np.isnan(np.array(U1)).nonzero()  \n",
    "        where_nan_v1 = np.isnan(np.array(V1)).nonzero()\n",
    "        where_nan_lh = np.isnan(np.array(LH)).nonzero()\n",
    "        where_nan_sh = np.isnan(np.array(SH)).nonzero()\n",
    "        where_nan_u2 = np.isnan(np.array(U2)).nonzero()\n",
    "        where_nan_v2 = np.isnan(np.array(V2)).nonzero()\n",
    "\n",
    "        # Fill NaNs with zeros in LH, SH, U, and V arrays \n",
    "        U1_temp_dt = U1.fillna(0.0)\n",
    "        V1_temp_dt = V1.fillna(0.0)\n",
    "        LH_temp_dt = LH.fillna(0.0)\n",
    "        SH_temp_dt = SH.fillna(0.0)\n",
    "        U2_temp_dt = U2.fillna(0.0)\n",
    "        V2_temp_dt = V2.fillna(0.0)\n",
    "    \n",
    "        # Detrend variables along time axis \n",
    "        gh1_dt = signal.detrend(GH1, axis=0)\n",
    "        gh2_dt = signal.detrend(GH2, axis=0)\n",
    "        u1_dt  = signal.detrend(U1_temp_dt, axis=0)\n",
    "        v1_dt  = signal.detrend(V1_temp_dt, axis=0)\n",
    "        lh_dt  = signal.detrend(LH_temp_dt, axis=0)\n",
    "        sh_dt  = signal.detrend(SH_temp_dt, axis=0)\n",
    "        u2_dt  = signal.detrend(U2_temp_dt, axis=0)\n",
    "        v2_dt  = signal.detrend(V2_temp_dt, axis=0)\n",
    "    \n",
    "        # Replace NaNs in LH, SH, U, and V arrays\n",
    "        u1_dt[where_nan_u1] = np.NaN\n",
    "        v1_dt[where_nan_v1] = np.NaN\n",
    "        lh_dt[where_nan_lh] = np.NaN\n",
    "        sh_dt[where_nan_sh] = np.NaN\n",
    "        u2_dt[where_nan_u2] = np.NaN\n",
    "        v2_dt[where_nan_v2] = np.NaN\n",
    "    ###################################\n",
    "       \n",
    "    ntime = len(T2M.coords['time.month'].values)\n",
    "        \n",
    "    if resolution==\"monthly\":\n",
    "        # Create date array which corresponds to 6 time steps/year\n",
    "        # Remove last time step to be consistent with non-lagged fields\n",
    "        # Note that this time array is not completely accurate as the time steps are divided into equal-length periods\n",
    "        # In reality, the periods are not equally spaced since there are gaps between six month periods\n",
    "        # This method works for plotting purposes, however\n",
    "        dates = np.linspace(0,nyears,ntime+1)[:-1] + beginyear\n",
    "    elif resolution==\"daily\":\n",
    "        # Create date array which corresponds to daily time steps for entire period\n",
    "        # Remove last time step to be consistent with non-lagged fields\n",
    "        dates = np.linspace(0,nyears,ntime+1)[:-1] + beginyear\n",
    "        \n",
    "        \n",
    "#     ################################### \n",
    "#     # -------------------------------- EOFs\n",
    "    \n",
    "#     # Calculate EOFs for pcp\n",
    "#     calc_pcp = calc_eof_w_function(lat_small, pcp_dt, 2,2, multi=False)\n",
    "#     # Get EOF 1 and 2 for pcp\n",
    "#     eof_pcp  = calc_pcp['eofs']\n",
    "#     # Print variance explained by each EOF\n",
    "#     print(calc_pcp['var'])\n",
    "#     # Plot EOfs 1 and 2 for pcp\n",
    "#     plot_eof(lon_small, lat_small, eof_pcp, 2, eoflim1,eoflim2, pcp=True, var=calc_pcp['var'][:2])\n",
    "#     plt.suptitle('PCP Standard EOF', size=20)\n",
    "#     plt.show() \n",
    "#     # Plot PCs 1 and 2 for pcp\n",
    "#     plot_pc(dates,calc_pcp['pcs'],ymin=-4.5,ymax=4.5)\n",
    "#     ################################### \n",
    "    \n",
    "    \n",
    "#     ################################### \n",
    "#     # Calculate EOFs for SM\n",
    "#     calc_sm = calc_eof_w_function(lat_small,sm_dt,2,2,multi=False)\n",
    "#     # Get EOF 1 and 2 for SM\n",
    "#     eof_sm  = calc_sm['eofs']\n",
    "#     # Print variance explained by each EOF \n",
    "#     print(calc_sm['var'])\n",
    "#     # Plot EOFs 1 and 2\n",
    "#     plot_eof(lon_small,lat_small,eof_sm,2,eoflim1,eoflim2,sm=True,var=calc_sm['var'][:2])\n",
    "#     plt.suptitle('SM Standard EOF', size=20)\n",
    "#     plt.show()\n",
    "#     # Plot PCs 1 and 2 for SM \n",
    "#     plot_pc(dates,calc_sm['pcs'],ymin=-4.5,ymax=4.5)\n",
    "#     ###################################\n",
    "    \n",
    "    \n",
    "    ###################################\n",
    "    # Calculate multivariate EOFs for SM, PCP, and T2M\n",
    "    # Use detrended time series\n",
    "    calc_smpcpt2m    = calc_eof_w_function(lat_small,[sm_dt,pcp_dt,t2m_dt],2,2,multi=True)\n",
    "    # Get EOFs 1 and 2 for each variable\n",
    "    eof_sm_smpcpt2m  = calc_smpcpt2m['eofs1']\n",
    "    eof_pcp_smpcpt2m = calc_smpcpt2m['eofs2']\n",
    "    eof_t2m_smpcpt2m = calc_smpcpt2m['eofs3']\n",
    "    # Get PC time series\n",
    "    pcs_smpcpt2m     = calc_smpcpt2m['pcs']\n",
    "    # Print variance explained by first two EOFs\n",
    "    #print(calc_smpcpt2m['var'])\n",
    "    \n",
    "    if opp:\n",
    "    # Multiply EOFs by -1 if desired\n",
    "        eof_sm_smpcpt2m  = -1*eof_sm_smpcpt2m\n",
    "        eof_pcp_smpcpt2m = -1*eof_pcp_smpcpt2m\n",
    "        eof_t2m_smpcpt2m = -1*eof_t2m_smpcpt2m\n",
    "    \n",
    "    # Plot EOF 1 and 2 for SM\n",
    "    plot_eof(lon_small,lat_small,eof_sm_smpcpt2m,2,meoflim1,meoflim2, sm=True, var=calc_smpcpt2m['var'])\n",
    "    #plt.suptitle('Multivariate EOFs - SM', size=20, y=0.98, fontname='Cantarell')\n",
    "    plt.savefig(filename + 'sm.svg')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot EOF 1 and 2 for pcp\n",
    "    plot_eof(lon_small,lat_small,eof_pcp_smpcpt2m,2,meoflim1,meoflim2, pcp=True, var=calc_smpcpt2m['var'])\n",
    "    #plt.suptitle('Multivariate EOFs - PCP', size=20, y=0.98, fontname='Cantarell')\n",
    "    plt.savefig(filename + 'pcp.svg')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot EOF 1 and 2 for T2M\n",
    "    plot_eof(lon_small,lat_small,eof_t2m_smpcpt2m,2,meoflim1,meoflim2, var=calc_smpcpt2m['var'])\n",
    "    #plt.suptitle('Multivariate EOFs - T2M', size=20, y=0.98, fontname='Cantarell')   \n",
    "    plt.savefig(filename + 't2m.svg')\n",
    "    plt.show()\n",
    "    \n",
    "    plot_pc(dates,calc_smpcpt2m['pcs'],ymin=-4.5,ymax=4.5)\n",
    "    plt.savefig(filename + 'mpcs.svg')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot variance accounted for by each month \n",
    "    #plot_monvar(calc_smpcpt2m['pcs'][:,0][2:])\n",
    "    #plot_monvar(calc_smpcpt2m['pcs'][:,1][2:])\n",
    "    \n",
    "    ###################################\n",
    "    \n",
    "    \n",
    "#     if ancillary:\n",
    "#     ###################################\n",
    "#         # Calculate multivariate EOFs for SM, LH, and T2M\n",
    "#         # Use detrended time series\n",
    "#         calc_smlht2m    = calc_eof_w_function(lat_small,[sm_dt,lh_dt,t2m_dt],2,2,multi=True)\n",
    "#         # Get EOFs 1 and 2 for each variable\n",
    "#         eof_sm_smlht2m  = calc_smlht2m['eofs1']\n",
    "#         eof_lh_smlht2m  = calc_smlht2m['eofs2']\n",
    "#         eof_t2m_smlht2m = calc_smlht2m['eofs3']\n",
    "#         # Get PC time series\n",
    "#         pcs_smlht2m     = calc_smlht2m['pcs']\n",
    "#         # Print variance explained by first two EOFs\n",
    "#         print(calc_smlht2m['var'])\n",
    "\n",
    "#         # Plot EOF 1 and 2 for SM\n",
    "#         plot_eof(lon_small,lat_small,eof_sm_smlht2m,2,meoflim1,meoflim2, sm=True, var=calc_smlht2m['var'][:2])\n",
    "#         #plt.suptitle('Multivariate EOFs - SM', size=20, y=0.98)\n",
    "#         plt.savefig(filename + 'sm_lh.png', dpi=300)\n",
    "#         plt.show()\n",
    "\n",
    "#         # Plot EOF 1 and 2 for LH\n",
    "#         plot_eof(lon_small,lat_small,eof_lh_smlht2m,2,meoflim1,meoflim2, var=calc_smlht2m['var'][:2])\n",
    "#         #plt.suptitle('Multivariate EOFs - LH', size=20, y=0.98)\n",
    "#         plt.savefig(filename + 'lh.png', dpi=300)\n",
    "#         plt.show()\n",
    "\n",
    "#         # Plot EOF 1 and 2 for T2M\n",
    "#         plot_eof(lon_small,lat_small,eof_t2m_smlht2m,2,meoflim1,meoflim2, var=calc_smlht2m['var'][:2])\n",
    "#         #plt.suptitle('Multivariate EOFs - T2M', size=20, y=0.98)\n",
    "#         plt.savefig(filename + 't2m_lh.png', dpi=300)\n",
    "#         plt.show()\n",
    "#         ###################################\n",
    "    \n",
    "    \n",
    "#         ###################################\n",
    "#         # Calculate multivariate EOFs for SM, SH, and T2M\n",
    "#         # Use detrended time series\n",
    "#         calc_smsht2m    = calc_eof_w_function(lat_small,[sm_dt,sh_dt,t2m_dt],2,2,multi=True)\n",
    "#         # Get EOFs 1 and 2 for each variable\n",
    "#         eof_sm_smsht2m  = calc_smsht2m['eofs1']\n",
    "#         eof_sh_smsht2m  = calc_smsht2m['eofs2']\n",
    "#         eof_t2m_smsht2m = calc_smsht2m['eofs3']\n",
    "#         # Get PC time series\n",
    "#         pcs_smsht2m     = calc_smsht2m['pcs']\n",
    "#         # Print variance explained by first two EOFs\n",
    "#         print(calc_smsht2m['var'])\n",
    "\n",
    "#         # Plot EOF 1 and 2 for SM\n",
    "#         plot_eof(lon_small,lat_small,eof_sm_smsht2m,2,meoflim1,meoflim2, sm=True, var=calc_smsht2m['var'][:2])\n",
    "#         #plt.suptitle('Multivariate EOFs - SM', size=20, y=0.98)\n",
    "#         plt.savefig(filename + 'sm_sh.png', dpi=300)\n",
    "#         plt.show()\n",
    "        \n",
    "#         # Plot EOF 1 and 2 for SH\n",
    "#         plot_eof(lon_small,lat_small,eof_sh_smsht2m,2,meoflim1,meoflim2, var=calc_smsht2m['var'][:2])\n",
    "#         #plt.suptitle('Multivariate EOFs - SH', size=20, y=0.98)\n",
    "#         plt.savefig(filename + 'sh.png', dpi=300)\n",
    "#         plt.show()\n",
    "\n",
    "#         # Plot EOF 1 and 2 for T2M\n",
    "#         plot_eof(lon_small,lat_small,eof_t2m_smsht2m,2,meoflim1,meoflim2, var=calc_smsht2m['var'][:2])\n",
    "#         #plt.suptitle('Multivariate EOFs - T2M', size=20, y=0.98)\n",
    "#         plt.savefig(filename + 't2m_sh.png', dpi=300)\n",
    "#         plt.show()\n",
    "        \n",
    "    ###################################\n",
    "    \n",
    "\n",
    "    ###################################\n",
    "    # -------------------------------- Calculate composites based on PC values\n",
    "    # Get PCs for EOF 1\n",
    "    pc1_smpcpt2m  = pcs_smpcpt2m[:,0]\n",
    "    \n",
    "    # Get PCs for EOF 2\n",
    "    pc2_smpcpt2m  = pcs_smpcpt2m[:,1]\n",
    "\n",
    "    # Get composites for time steps above and below 1 std\n",
    "    #t2m_comp_p_m1, t2m_comp_n_m1 = calc_comp(t2m_dt, pc1_smpcpt2m)\n",
    "    #sm_comp_p_m1, sm_comp_n_m1   = calc_comp(sm_dt, pc1_smpcpt2m)  \n",
    "    #pcp_comp_p_m1, pcp_comp_n_m1 = calc_comp(pcp_dt, pc1_smpcpt2m)\n",
    "    \n",
    "    #t2m_comp_p_m2, t2m_comp_n_m2 = calc_comp(t2m_dt, pc2_smpcpt2m)\n",
    "    #sm_comp_p_m2, sm_comp_n_m2   = calc_comp(sm_dt, pc2_smpcpt2m)  \n",
    "    #pcp_comp_p_m2, pcp_comp_n_m2 = calc_comp(pcp_dt, pc2_smpcpt2m)\n",
    "    \n",
    "    if ancillary:\n",
    "#         gh1_comp_p_m1, gh1_comp_n_m1 = calc_comp(gh1_dt, pc1_smpcpt2m)\n",
    "#         gh2_comp_p_m1, gh2_comp_n_m1 = calc_comp(gh2_dt, pc1_smpcpt2m)\n",
    "#         U1_comp_p_m1, U1_comp_n_m1   = calc_comp(u1_dt, pc1_smpcpt2m)\n",
    "#         V1_comp_p_m1, V1_comp_n_m1   = calc_comp(v1_dt, pc1_smpcpt2m)\n",
    "        lh_comp_p_m1, lh_comp_n_m1   = calc_comp(lh_dt, pc1_smpcpt2m) \n",
    "        sh_comp_p_m1, sh_comp_n_m1   = calc_comp(sh_dt, pc1_smpcpt2m) \n",
    "#         U2_comp_p_m1, U2_comp_n_m1   = calc_comp(u2_dt, pc1_smpcpt2m)\n",
    "#         V2_comp_p_m1, V2_comp_n_m1   = calc_comp(v2_dt, pc1_smpcpt2m)\n",
    "\n",
    "#         gh1_comp_p_m2, gh1_comp_n_m2 = calc_comp(gh1_dt, pc2_smpcpt2m)\n",
    "#         gh2_comp_p_m2, gh2_comp_n_m2 = calc_comp(gh2_dt, pc2_smpcpt2m)\n",
    "#         U1_comp_p_m2, U1_comp_n_m2   = calc_comp(u1_dt, pc2_smpcpt2m)\n",
    "#         V1_comp_p_m2, V1_comp_n_m2   = calc_comp(v1_dt, pc2_smpcpt2m)\n",
    "        lh_comp_p_m2, lh_comp_n_m2   = calc_comp(lh_dt, pc2_smpcpt2m) \n",
    "        sh_comp_p_m2, sh_comp_n_m2   = calc_comp(sh_dt, pc2_smpcpt2m) \n",
    "#         U2_comp_p_m2, U2_comp_n_m2   = calc_comp(u2_dt, pc2_smpcpt2m)\n",
    "#         V2_comp_p_m2, V2_comp_n_m2   = calc_comp(v2_dt, pc2_smpcpt2m)\n",
    "    ###################################\n",
    "\n",
    "\n",
    "    ###################################\n",
    "    # -------------------------------- Regrid wind data\n",
    "#         grid_in = {'lon': lon_large,\n",
    "#                'lat': lat_large}\n",
    "\n",
    "#         grid_out = {'lon': lon_large[::6],\n",
    "#                 'lat': lat_large[::6]}\n",
    "\n",
    "#         regridder = xe.Regridder(grid_in, grid_out, 'bilinear')\n",
    "\n",
    "#         U1_comp_p_m1 = regridder(U1_comp_p_m1)\n",
    "#         V1_comp_p_m1 = regridder(V1_comp_p_m1)\n",
    "#         U1_comp_n_m1 = regridder(U1_comp_n_m1)\n",
    "#         V1_comp_n_m1 = regridder(V1_comp_n_m1)\n",
    "\n",
    "#         U1_comp_p_m2 = regridder(U1_comp_p_m2)\n",
    "#         V1_comp_p_m2 = regridder(V1_comp_p_m2)\n",
    "#         U1_comp_n_m2 = regridder(U1_comp_n_m2)\n",
    "#         V1_comp_n_m2 = regridder(V1_comp_n_m2)\n",
    "\n",
    "#         U2_comp_p_m1 = regridder(U2_comp_p_m1)\n",
    "#         V2_comp_p_m1 = regridder(V2_comp_p_m1)\n",
    "#         U2_comp_n_m1 = regridder(U2_comp_n_m1)\n",
    "#         V2_comp_n_m1 = regridder(V2_comp_n_m1)\n",
    "\n",
    "#         U2_comp_p_m2 = regridder(U2_comp_p_m2)\n",
    "#         V2_comp_p_m2 = regridder(V2_comp_p_m2)\n",
    "#         U2_comp_n_m2 = regridder(U2_comp_n_m2)\n",
    "#         V2_comp_n_m2 = regridder(V2_comp_n_m2)\n",
    "\n",
    "#         regridder.clean_weight_file()\n",
    "    ###################################\n",
    "        \n",
    "    \n",
    "    ###################################\n",
    "    # -------------------------------- Plot PC composites\n",
    "    #plot_comp(lon_small,lat_small,t2m_comp_p_m1,t2m_comp_n_m1,cmap='bwr',eofnum=1,vmin=clim1,vmax=clim2)\n",
    "    \n",
    "    #plot_comp(lon_small,lat_small,sm_comp_p_m1,sm_comp_n_m1,cmap='BrBG',eofnum=1,vmin=clim1,vmax=clim2)\n",
    "    \n",
    "    #plot_comp(lon_small,lat_small,pcp_comp_p_m1,pcp_comp_n_m1,cmap='PRGn',eofnum=1,vmin=clim1,vmax=clim2)\n",
    "    \n",
    "    \n",
    "    #plot_comp(lon_small,lat_small,t2m_comp_p_m2,t2m_comp_n_m2,cmap='bwr',eofnum=2, vmin=clim1,vmax=clim2)\n",
    "    \n",
    "    #plot_comp(lon_small,lat_small,sm_comp_p_m2,sm_comp_n_m2,cmap='BrBG',eofnum=2, vmin=clim1,vmax=clim2)\n",
    "    \n",
    "    #plot_comp(lon_small,lat_small,pcp_comp_p_m2,pcp_comp_n_m2,cmap='PRGn',eofnum=2, vmin=clim1,vmax=clim2)\n",
    "    \n",
    "    \n",
    "    if ancillary:\n",
    "        #plot_comp(lon_large, lat_large, gh1_comp_p_m1, gh1_comp_n_m1, title='Lag 0', cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#131DBF\",\"white\",\"#C91717\"]), \n",
    "        #          eofnum=1, interval=0.155, ticks=[-1,-0.545,0,0.545,1], U=[U1_comp_p_m1, U1_comp_n_m1], V=[V1_comp_p_m1, V1_comp_n_m1], vmin=clim1,vmax=clim2, filename=filename+'eof1gh1')\n",
    "\n",
    "        plot_comp(lon_small,lat_small,lh_comp_p_m1,lh_comp_n_m1,title='',cmap=matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#131DBF\",\"white\",\"#C91717\"]),\n",
    "                  eofnum=1,interval=0.165, ticks=[-1,-0.545,0,0.545,1], vmin=clim1,vmax=clim2, filename=filename+'eof1_lh')\n",
    "\n",
    "        plot_comp(lon_small,lat_small,sh_comp_p_m1,sh_comp_n_m1,title='',cmap=matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#131DBF\",\"white\",\"#C91717\"]),\n",
    "                  eofnum=1,interval=0.165, ticks=[-1,-0.545,0,0.545,1], vmin=clim1,vmax=clim2, filename=filename+'eof1_sh')\n",
    "        \n",
    "        #plot_comp(lon_large, lat_large, gh2_comp_p_m1, gh2_comp_n_m1, title='Lag 1', cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#131DBF\",\"white\",\"#C91717\"]),\n",
    "        #          eofnum=1, interval=0.155, ticks=[-1,-0.545,0,0.545,1], U=[U2_comp_p_m1, U2_comp_n_m1], V=[V2_comp_p_m1, V2_comp_n_m1], vmin=clim1,vmax=clim2, filename=filename+'eof1gh2')\n",
    "\n",
    "        \n",
    "        #plot_comp(lon_large, lat_large, gh1_comp_p_m2, gh1_comp_n_m2, title='Lag 0', cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#131DBF\",\"white\",\"#C91717\"]),\n",
    "        #          eofnum=2, interval=0.155, ticks=[-1,-0.545,0,0.545,1], U=[U1_comp_p_m2, U1_comp_n_m2], V=[V1_comp_p_m2, V1_comp_n_m2], vmin=clim1,vmax=clim2, filename=filename+'eof2gh1')\n",
    "\n",
    "        plot_comp(lon_small,lat_small,lh_comp_p_m2,lh_comp_n_m2,title='',cmap=matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#131DBF\",\"white\",\"#C91717\"]),\n",
    "                  eofnum=2,interval=0.165, ticks=[-1,-0.545,0,0.545,1], vmin=clim1,vmax=clim2, filename=filename+'eof2_lh')\n",
    "\n",
    "        plot_comp(lon_small,lat_small,sh_comp_p_m2,sh_comp_n_m2,title='',cmap=matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#131DBF\",\"white\",\"#C91717\"]),\n",
    "                  eofnum=2,interval=0.165, ticks=[-1,-0.545,0,0.545,1], vmin=clim1,vmax=clim2, filename=filename+'eof2_sh')\n",
    "\n",
    "        #plot_comp(lon_large, lat_large, gh2_comp_p_m2, gh2_comp_n_m2, title='Lag 1', cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"#131DBF\",\"white\",\"#C91717\"]),\n",
    "        #          eofnum=2, interval=0.155, ticks=[-1,-0.545,0,0.545,1], U=[U2_comp_p_m2, U2_comp_n_m2], V=[V2_comp_p_m2, V2_comp_n_m2], vmin=clim1,vmax=clim2, filename=filename+'eof2gh2')\n",
    "    ###################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
