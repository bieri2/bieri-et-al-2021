{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from eofs.standard import Eof\n",
    "from eofs.multivariate.standard import MultivariateEof\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read ERA5 data \n",
    "def read_era5(variable, subset=[240.0,15.0,-80.0,20.0], plot=True, SM=False, passpm=False):\n",
    "    # Define ERA5 directory and file name depending on variable\n",
    "    era5_dir  = '/glade/work/cab478/era5'\n",
    "    era5_file = era5_dir + '/era5_' + variable + '_1980_2016.nc'\n",
    "    \n",
    "    # Get ERA5 variable name (which may be different than abbreviation used in file name)\n",
    "    if variable=='precip':\n",
    "        varname = 'tp'\n",
    "    elif variable=='sm':\n",
    "        varname = ['swvl1','swvl2','swvl3']\n",
    "    elif variable=='t2m':\n",
    "        varname = variable\n",
    "    \n",
    "    # Let user know which file is being read\n",
    "    print(era5_file)\n",
    "    \n",
    "    # Define pandas date range with monthly frequency\n",
    "    dates = pd.date_range(start='1/1/1980', end='12/31/2016', freq='M')\n",
    "    # Exclude months not in spring/summer\n",
    "    dates = dates[(dates.month>=9) | (dates.month<=3)]\n",
    "\n",
    "    # Read ERA5 data from file\n",
    "    data  = Dataset(era5_file)\n",
    "    \n",
    "    # Convert lat and lon arrays to Series\n",
    "    lat = pd.Series(data.variables['latitude'][:])\n",
    "    lon = pd.Series(data.variables['longitude'][:])\n",
    "    \n",
    "    # Do this if given a longitude range that passes the Prime Meridian (0 deg lon)\n",
    "    if passpm:\n",
    "        # Get indices of points up to the Prime Meridian\n",
    "        lon_slice1 = lon[lon>subset[0]].index.tolist()\n",
    "        # Get indices of points beyond the Prime Meridian\n",
    "        lon_slice2 = lon[lon<subset[1]].index.tolist()\n",
    "    # Do this in all other cases\n",
    "    else:\n",
    "        lon_slice  = lon[(lon>subset[0])&(lon<subset[1])].index.tolist()\n",
    "    \n",
    "    # Get indices of data within given latitude bounds\n",
    "    lat_slice = lat[(lat>subset[2])&(lat<subset[3])].index.tolist()\n",
    "    \n",
    "    # Do this if SM data is desired\n",
    "    if SM:\n",
    "        if passpm:\n",
    "    \n",
    "            # Read in SM data for each ERA5 soil layer (top 3 layers only)\n",
    "            # Concatenate data for both longitude subsets\n",
    "            L1 = np.concatenate((data.variables[varname[0]][:,lat_slice,lon_slice1],\n",
    "                                     data.variables[varname[0]][:,lat_slice,lon_slice2]),axis=2)\n",
    "            \n",
    "            L2 = np.concatenate((data.variables[varname[1]][:,lat_slice,lon_slice1],\n",
    "                                     data.variables[varname[1]][:,lat_slice,lon_slice2]),axis=2)\n",
    "    \n",
    "            L3 = np.concatenate((data.variables[varname[2]][:,lat_slice,lon_slice1],\n",
    "                                     data.variables[varname[2]][:,lat_slice,lon_slice2]),axis=2)\n",
    "        # Do this in all other cases\n",
    "        else:\n",
    "            L1 = data.variables[varname[0]][:,lat_slice,lon_slice]\n",
    "            L2 = data.variables[varname[1]][:,lat_slice,lon_slice]\n",
    "            L3 = data.variables[varname[2]][:,lat_slice,lon_slice]\n",
    "            \n",
    "        # Set missing data to NaNs (very small SM values)\n",
    "        L1[L1 < 0.0001] = np.NaN\n",
    "        L2[L2 < 0.0001] = np.NaN\n",
    "        L3[L3 < 0.0001] = np.NaN\n",
    "        \n",
    "        # Concatenate data for all layers\n",
    "        all_l  = np.concatenate((L1[...,np.newaxis], L2[...,np.newaxis]), axis=3)\n",
    "        all_l  = np.concatenate((all_l, L3[...,np.newaxis]), axis=3)\n",
    "    \n",
    "        # Create masked array before computing weighted average \n",
    "        masked_all_l = np.ma.masked_array(all_l, np.isnan(all_l))\n",
    "        \n",
    "        # Define array of ERA5 soil layer thicknesses\n",
    "        # From https://confluence.ecmwf.int/pages/viewpage.action?pageId=56660259    \n",
    "        thick = np.array([0.07,0.21,0.72])\n",
    "        # Compute layer thickness weights \n",
    "        wgts  = thick/thick.sum()\n",
    "        # Perform weighted average on masked array using weights\n",
    "        wgt_avg = np.ma.average(masked_all_l,axis=3,weights=wgts)\n",
    "        # Replace masked values with NaNs\n",
    "        out     = wgt_avg.filled(np.NaN)\n",
    "    \n",
    "    # Do this if SM data not desired\n",
    "    else:\n",
    "        # Do this if given longitude values include Prime Meridian\n",
    "        if passpm:\n",
    "            out = np.concatenate((data.variables[varname][:,lat_slice,lon_slice1],\n",
    "                                  data.variables[varname][:,lat_slice,lon_slice2]),axis=2)\n",
    "        else:\n",
    "            out = data.variables[varname][:,lat_slice,lon_slice]\n",
    "        \n",
    "        # Calculate fill value using scale factor and offset value\n",
    "        new_fv  = (data.variables[varname]._FillValue*data.variables[varname].scale_factor) + \\\n",
    "                   data.variables[varname].add_offset\n",
    "        # Calculate difference between data array and calculated fill value\n",
    "        fv_diff = abs(out-new_fv)\n",
    "        # If this difference is equal to or less than a threshold, set those values in data array to NaN\n",
    "        out[fv_diff <= 0.01] = np.NaN\n",
    "        print(new_fv)\n",
    "        \n",
    "    # Define xarray data arrays to hold output data and coordinate information\n",
    "    # In both cases, the convention of the longitude array is changed from 0 to 360 to -180 to 180\n",
    "    if passpm:\n",
    "        lon_values = np.asarray(pd.concat((lon[lon_slice1]-360.,lon[lon_slice2]))) \n",
    "        array      = xr.DataArray(out, coords={'time':dates,'lat':np.array(lat[lat_slice].values), \n",
    "                                               'lon':lon_values}, dims=['time','lat','lon'])\n",
    "    else:\n",
    "        lon_values = np.asarray(lon[lon_slice])\n",
    "        lon_values[lon_values>=180] = lon_values[lon_values>=180] - 360.\n",
    "        array = xr.DataArray(out, coords={'time':dates,'lat':np.array(lat[lat_slice].values), \n",
    "                                          'lon':np.array(lon_values)}, dims=['time','lat','lon'])\n",
    "        \n",
    "    if plot:\n",
    "        # Plot first time step to check data\n",
    "        fig = plt.figure(figsize=(10, 6), edgecolor='w')\n",
    "        m = Basemap(projection='cyl',\n",
    "                llcrnrlat=-90, urcrnrlat=90,\n",
    "                llcrnrlon=-180, urcrnrlon=180)\n",
    "        lon, lat = np.meshgrid(lon_values, lat[lat_slice])\n",
    "        m.pcolormesh(lon, lat, out[0,:,:],\n",
    "             latlon=True, cmap='plasma')\n",
    "        m.drawcoastlines()\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "    \n",
    "    return array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
